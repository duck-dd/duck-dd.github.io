<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CS on Duck</title>
    <link>https://duck-dd.github.io/categories/cs/</link>
    <description>Recent content in CS on Duck</description>
    <image>
      <title>Duck</title>
      <url>https://duck-dd.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://duck-dd.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.8</generator>
    <language>en</language>
    <lastBuildDate>Tue, 24 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://duck-dd.github.io/categories/cs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MCMF from scratch</title>
      <link>https://duck-dd.github.io/posts/mcmf/</link>
      <pubDate>Tue, 24 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://duck-dd.github.io/posts/mcmf/</guid>
      <description>&lt;p&gt;需要深度用到MCMF了，但是摸了摸自己的脑门，脑子里没有公式和演算，只剩几个概念了。
记录一下我从使用的角度理解MCMF问题(东拼西凑)的过程；某些定理的推导，或者复杂度的计算原理，不会深扒；顺便说个感悟，在feed如此丰富的今天，我们还是应该时刻锻炼传统手艺，保留信息检索、过滤、沉淀的能力，否则可能会越走越远(挺简单个问题一开始看偏了太难理解了)。&lt;/p&gt;
&lt;h1 id=&#34;mcmf概念&#34;&gt;MCMF概念&lt;/h1&gt;
&lt;p&gt;Minimum Cost Maximum Flow(最小费用最大流)，满足流量约束前提下，找到源点到汇点的最大流，并使总运输费用最小，数学模型如下：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;有向图 G=(V,E)，每条边 e=(u,v) 包含容量 c(e) 和单位流量费用 w(e)；&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;求源点 s 到汇点 t 的最大流，且总费用 ∑w(e)⋅f(e) 最小，其中 f(e) 为边 e 上的流量&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在解析MCMF之前，我们先一起了解一下他的前身，最大流问题和最小费用流问题。&lt;/p&gt;
&lt;h1 id=&#34;最大流问题&#34;&gt;最大流问题&lt;/h1&gt;
&lt;p&gt;最大流问题是：在一个有向网络中，找到从源点（流量的起点）到汇点（流量的终点）的最大可行流量，同时满足每条边的容量限制；该问题是上世纪五十年代提出的，提出后Lester Ford和Delbert Fulkerson很快给出了解法，也是最大流后续一切发展的理论基础：&lt;code&gt;Ford-Fulkerson&lt;/code&gt;算法。&lt;/p&gt;
&lt;h2 id=&#34;第一印象&#34;&gt;第一印象&lt;/h2&gt;
&lt;p&gt;简单描述，有向有权图，起点&lt;code&gt;s&lt;/code&gt;，终点&lt;code&gt;t&lt;/code&gt;，我们要寻找&lt;code&gt;s-&amp;gt;t&lt;/code&gt;的最大的流量。
那么朴素的第一印象来看，暴力呗，BFS把所有&lt;code&gt;s-&amp;gt;t&lt;/code&gt;路径集合&lt;code&gt;P&lt;/code&gt;全部记录下来，然后逐条路径遍历&lt;code&gt;P&lt;/code&gt;，每一条路径都跑满(路径上最小容量边)，同时更新涉及的边的容量值(这里可能还需要维护一个&lt;code&gt;边-&amp;gt;剩余容量&lt;/code&gt;)，结束后得到一个总的流量值。
提出这个方案后，我们第一时间就会有做简单优化的想法，最朴素的想法就是，对于&lt;code&gt;s&lt;/code&gt;的所有出边都满，或者&lt;code&gt;t&lt;/code&gt;的所有入边都满的情况下，是可以快速退出的；有了快速退出这个想法后，继续思考，&lt;code&gt;P&lt;/code&gt;的遍历顺序是对退出速度有影响的，初始状态下所有边没有消耗，&lt;code&gt;P&lt;/code&gt;中的所有路径按大小排序，从大到小的顺序排序，大点干早点散(每次选择一条路径并记录开销后，也可以对&lt;code&gt;P&lt;/code&gt;剩余的路径重排序，只不过开销比较大)，这样感觉上可以更快结束。
好了，聪明的你，在10s以内经历了上述的思考过程，但你眉头一皱，发现问题并不如此简单，简单的想法并不能确保能找到最大流，例如如下的情况：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;图1 blocking-flow(黑色数字=容量,红色数字=流量)&#34; loading=&#34;lazy&#34; src=&#34;https://duck-dd.github.io/images/mcmf/mcmf-1.png&#34;&gt;&lt;/p&gt;
&lt;center&gt;图1 blocking-flow(黑色数字=容量,红色数字=流量)&lt;/center&gt;
&lt;p&gt;如图1左(黑色数字=容量,红色数字=流量)，流量=4后，无法再找到路径可通过流量了，但是如图1右，最大流其实是5；这里引入一个概念，&lt;code&gt;阻塞流&lt;/code&gt;，即将所有的路径都阻塞了，无法再新增流量；显而易见，最大流是阻塞流，但阻塞流未必是最大流。
结合这个具体的例子，我们反过来思考为什么我们上面一起想出来的朴素的方法无法确保能够找到最大流呢？我们就结合这一个具体的简单的例子来分析，把整个图1左的中间部分完全忽略，我们只关注起点&lt;code&gt;s&lt;/code&gt;以及他的出边，终点&lt;code&gt;t&lt;/code&gt;以及他的入边，我们碰到了这样一个情况，我们选定了&lt;code&gt;s&lt;/code&gt;的左出边以及&lt;code&gt;t&lt;/code&gt;的右入边并且把它们两个给跑满了，而&lt;code&gt;s&lt;/code&gt;的右出边和&lt;code&gt;t&lt;/code&gt;的左入边他们两个虽然还有容量但是却无法互连(即我们忽略的图的中间的部分没办法把它们连接起来)，而图1右中，&lt;code&gt;t&lt;/code&gt;的右入边跑满的流量不仅仅来自于&lt;code&gt;s&lt;/code&gt;的左出边，还有&lt;code&gt;s&lt;/code&gt;的右出边；ok, you got it! 简单总结，我们的朴素的暴力解法存在的问题是，会存在不合理的路径规划，他会把若干个瓶颈边放到一条路径里，导致多个瓶颈被同时耗尽。当然了，这是我们最通俗的理解，未必描述的是准确的。&lt;/p&gt;
&lt;p&gt;那么如何修正能够确保准确的找到最大流呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;再继续暴力的把&lt;code&gt;P&lt;/code&gt;的所有排序都跑一遍，复杂度不太现实(感兴趣可以算算复杂度)&lt;/li&gt;
&lt;li&gt;前面提到问题在于某一些路径的选取不合理，它可能同时触发了多个瓶颈(它本可以少触发一些瓶颈)
&lt;ul&gt;
&lt;li&gt;我们避免使用到不合理的路径？那对路径打分？结合实时的 &lt;code&gt;residual graph(残量图，余量图，残量网络 等)&lt;/code&gt;，对剩余的路径打分，路径消耗掉的边(消耗掉边就是指一条边跑满)越少分越高？也不合理，每一条边价值并不是相等的，例如跨海大桥，这一条边甚至就是整个图的瓶颈；那我们从点入手？每个点都有流入边和流出边，流入和流出在每个点是相等的(&lt;code&gt;s&lt;/code&gt; &lt;code&gt;t&lt;/code&gt;除外)，我们尽量让每个点的流入流出比接近于他的容量的入出比，并对路径的所有点做加权后作为评分？说实话我不知道这个想法合理不合理，但是只要是想要依赖&lt;code&gt;residual graph&lt;/code&gt;来建立评分机制，那么随着迭代次数提升，每次都要更新评分，复杂度应该都是不可接受的&lt;/li&gt;
&lt;li&gt;那么换个思路，我们不在避免使用到不合理路径上下功夫，我们能不能做到随时撤销之前的不合理路径？这样我们就可以大胆的随便搞，一边搞一边修正直至结束；恭喜你，你跟Lester Ford和Delbert Fulkerson可能想到一块去了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ford-fulkerson&#34;&gt;Ford-Fulkerson&lt;/h2&gt;
&lt;p&gt;一句话描述&lt;code&gt;Ford-Fulkerson&lt;/code&gt;算法，就是在建立&lt;code&gt;residual graph&lt;/code&gt;时，除了更新每条边的残余容量，还会对已经产生的流量建立反向边，下一轮迭代时，反向边也可以使用。
从物理意义上，反向边一开始是没有的，对正向边开销后才会有反向边(容量等于正向边的开销值)，这没有问题；反向边产生开销时，实际效果类似于水流对冲，本来正向走3个水流，反向再走一个水流，其实最终的效果就等于这条水管(边)正向走了2个水流，解释的通，看来可以理解。
但是回到上面我们自己的思考，我们(或者可能只是我)愚蠢的脑袋里想的是要对之前的路径做撤销，当我们在某一轮迭代中使用到了一条反向边时，我们相当于对曾经使用到这条边的某一条路径撤销了一个流量，是这样吗？(如果是这样那就不应该仅是这一条反向边要做开销了，而是应该找到一条路径)
不是的，还是用图1左举例，他的&lt;code&gt;residual graph&lt;/code&gt;如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;图2 图1左的residual graph&#34; loading=&#34;lazy&#34; src=&#34;https://duck-dd.github.io/images/mcmf/mcmf-2.png&#34;&gt;&lt;/p&gt;
&lt;center&gt;图2 图1左部的residual-graph&lt;/center&gt;
&lt;p&gt;图2中，我们可以继续找到这样一条路径&lt;code&gt;s-&amp;gt;v2-&amp;gt;v4-&amp;gt;v1-&amp;gt;v3-&amp;gt;t 流量=1&lt;/code&gt;，其中&lt;code&gt;v4-&amp;gt;v1 流量=1&lt;/code&gt;这一段是我们选中的反向边(我们只对这一条边做了&amp;quot;撤销&amp;quot;)，可以看到，经过这一次修正后，我们实际的路径选择就跟图1右的最大流一致了，眼前的事实证明&lt;code&gt;Ford-Fulkerson&lt;/code&gt;算法是正确的。
那我们尝试解答一下我们刚才产生的那个疑惑，当我们使用了一条反向边的时候，我们究竟在做什么?我是这样理解这个问题的，在&lt;code&gt;residual graph&lt;/code&gt;中，如果我们在一条路径中使用到了一条反向边，那么说明一个问题，在最初的原图中，分别存在&lt;code&gt;s&lt;/code&gt;&amp;mdash;&amp;gt;&lt;code&gt;t&lt;/code&gt;的这样两条路径，分别包含了这条边的两个端点，我们可以把这个图抽象成一个&lt;code&gt;H&lt;/code&gt;型，这条边就是中间的横杠，当我们开销这条边的反向边时，实际是在调整&lt;code&gt;H&lt;/code&gt;型的两条竖线之间的流量分配方式(即合理规划使用&lt;code&gt;H&lt;/code&gt;的左上 左下 右上 右下四部分)，以使得整个&lt;code&gt;H&lt;/code&gt;通过的流量最大。&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Shortest Path</title>
      <link>https://duck-dd.github.io/posts/shortest-path/</link>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://duck-dd.github.io/posts/shortest-path/</guid>
      <description>&lt;h1 id=&#34;最短路径问题&#34;&gt;最短路径问题&lt;/h1&gt;
&lt;p&gt;寻找有向图中两个顶点之间的路径，使得 &lt;code&gt;路径最短&lt;/code&gt; 或 &lt;code&gt;路径上各边的权重之和最小&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;1-无权图最短路径&#34;&gt;1 无权图最短路径&lt;/h2&gt;
&lt;p&gt;所有边权重相同，最短路径退化为寻找两点间边数最少的路径，&lt;code&gt;BFS&lt;/code&gt;搞定。&lt;/p&gt;
&lt;h2 id=&#34;2-单源最短路径sssp-single-source-shortest-paths&#34;&gt;2 单源最短路径(SSSP, Single-Source Shortest Paths)&lt;/h2&gt;
&lt;h3 id=&#34;21-dijkstra堆优化&#34;&gt;2.1 Dijkstra(堆优化)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;限制：边权非负&lt;/li&gt;
&lt;li&gt;思路：基于已经确定的最短路径，逐步贪心获得源点到所有节点的最短路径&lt;/li&gt;
&lt;li&gt;步骤
&lt;ul&gt;
&lt;li&gt;初始化：
&lt;ul&gt;
&lt;li&gt;源点&lt;code&gt;s&lt;/code&gt;的距离为0，源点&lt;code&gt;s&lt;/code&gt;一步可达的节点距离记为单边的权重，其他节点距离为无穷大(∞)&lt;/li&gt;
&lt;li&gt;所有节点标记为未访问&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;循环处理：
&lt;ul&gt;
&lt;li&gt;从未访问节点中选择距离最小的节点&lt;code&gt;u&lt;/code&gt;，标记为已访问&lt;/li&gt;
&lt;li&gt;对&lt;code&gt;u&lt;/code&gt;的每个邻接节点&lt;code&gt;v&lt;/code&gt;，进行 &lt;strong&gt;松弛操作&lt;/strong&gt;:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;if distance[v] &amp;gt; distance[u] + weight(u, v):
    distance[v] = distance[u] + weight(u, v)
    predecessor[v] = u  # 记录路径
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;终止条件：所有节点均被访问，或未访问节点的最小距离为 ∞（表示源点无法到达剩余节点）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;优化：使用优先队列(最小堆)维护未访问节点，每次提取最小距离节点的时间为&lt;code&gt;O(logV)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;记录路径：通过记录&lt;code&gt;前驱节点&lt;/code&gt;可以完整还原最短路径，若存在多条最短路径(距离相同但路径不同)，会记录其中一条(具体取决于节点的访问顺序，例如堆优化中相同距离节点的出堆顺序)&lt;/li&gt;
&lt;li&gt;算法正确性理解：未访问节点中排序最靠前的(距离最小的)节点，是基于所有已访问节点推算出来的最近的点，如果再绕行其他节点，那么一定比当前距离更远；相反，如果不是未访问节点里距离最小的点，可能通过其他未访问节点绕行更优，所以每次迭代可以标记这一个点&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22-bellman-ford&#34;&gt;2.2 Bellman-Ford&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;限制：边权可以为负，但不能有从源点可达的负权环(否则最短路径无意义，长度可无限小)&lt;/li&gt;
&lt;li&gt;思路：通过 &lt;strong&gt;松弛操作&lt;/strong&gt; 逐步逼近从源点到所有其他顶点的最短路径；松弛操作指的是：对于每条边&lt;code&gt;(u, v)&lt;/code&gt;，若从源点到&lt;code&gt;u&lt;/code&gt;的距离 &lt;code&gt;dist[u]&lt;/code&gt;加上边权&lt;code&gt;w(u, v)&lt;/code&gt;小于当前到&lt;code&gt;v&lt;/code&gt;的距离&lt;code&gt;dist[v]&lt;/code&gt;，则更新&lt;code&gt;dist[v]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;步骤
&lt;ul&gt;
&lt;li&gt;初始化：源点距离&lt;code&gt;dist[source] = 0&lt;/code&gt;，其他顶点距离&lt;code&gt;dist[v] = ∞&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;弛操作：对图中所有边进行&lt;code&gt;n-1&lt;/code&gt;轮松弛（&lt;code&gt;n&lt;/code&gt;为顶点数）；因为最短路径最多包含&lt;code&gt;n-1&lt;/code&gt;条边(否则存在环，若为正权环可忽略，负权环则无法求解)&lt;/li&gt;
&lt;li&gt;检测负权环：第&lt;code&gt;n&lt;/code&gt;次松弛时，若仍能更新距离，则说明存在从源点可达的负权环&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;优化：下面的SPFA&lt;/li&gt;
&lt;li&gt;算法正确性理解：&lt;code&gt;n&lt;/code&gt;个节点，那么起点到终点路径最长就是 &lt;code&gt;1-&amp;gt;2-&amp;gt;3-&amp;gt;...-&amp;gt;n&lt;/code&gt; 最多有&lt;code&gt;n-1&lt;/code&gt;跳(边)，否则的话就是有环了(如果是正环，绕行是更差的解，如果是负环，最短路径无解)，算法迭代&lt;code&gt;x&lt;/code&gt;轮，那么&lt;code&gt;x&lt;/code&gt;跳能到达的节点的最短路径都会被优化完成，所以经过&lt;code&gt;n-1&lt;/code&gt;轮迭代，最长的路径也能被优化完成了；如何理解这句话呢，假设一个点距离起点有1条边和3条边两条路径，那么经过三轮迭代，这两条路径之间一定会做PK，择优就会完成&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;23-spfa-shortest-path-faster-algorithm&#34;&gt;2.3 SPFA, Shortest Path Faster Algorithm&lt;/h3&gt;
&lt;p&gt;SPFA其实只是Bellman-Ford的筛选优化，本质相同&lt;/p&gt;</description>
    </item>
    <item>
      <title>Golang Tips</title>
      <link>https://duck-dd.github.io/posts/go-tips/</link>
      <pubDate>Sun, 22 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://duck-dd.github.io/posts/go-tips/</guid>
      <description>&lt;p&gt;说明：&lt;/p&gt;
&lt;p&gt;持续更新(标题含TODO关键字的小节，都会以topic开始，后续会持续完善topic)。记录内容是一些对我而言：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;小的骚操作(可能有些tricky)&lt;/li&gt;
&lt;li&gt;容易理解偏差的点&lt;/li&gt;
&lt;li&gt;冷门的点(没啥用的点)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;golang代码执行顺序&#34;&gt;Golang代码执行顺序&lt;/h2&gt;
&lt;p&gt;没有并发，一个顺序逻辑，CPU真正执行指令不一定与编码顺序完全一致，Go的编译器会做优化，前提是会解决依赖逻辑，看起来是“顺序执行”这一假设。&lt;/p&gt;
&lt;p&gt;了解更多可查看Golang内存模型规范。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;internal包&#34;&gt;internal包&lt;/h2&gt;
&lt;p&gt;internal包，只能被和internal目录有同一个父目录的包所导入。
例如，net/http/internal/chunked内部包只能被net/http/httputil或net/http包导入，但是不能被net/url包导入。不过net/url包却可以导入net/http/httputil包。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;变量交换&#34;&gt;变量交换&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;i, j = j, i // 交换 i 和 j 的值
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h2 id=&#34;for循环有其他识别break的关键字&#34;&gt;for循环有其他识别break的关键字&lt;/h2&gt;
&lt;p&gt;for循环内有其他识别break的关键字时，其他关键字内的break会被其识别而不会跳出for，以下用select举例，switch同理。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;for {
		select {
		case &amp;lt;-sigChan:
			// exit for  
			break
		default:
		  // do something
		}
	}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上break并不能退出for循环，可以使用标签或goto解决：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;// 1 标签
FOR:
	for {
		select {
		case &amp;lt;-sigChan:
			// exit for  
			break FOR
		default:
		  // do something
		}
	}

----------------------------
// 2 goto
	for {
		select {
		case &amp;lt;-sigChan:
			// exit for  
			goto ENDFOR
		default:
		  // do something
		}
	}
ENDFOR:
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h2 id=&#34;读取stdin刷题别再因为stdin踩坑了喂&#34;&gt;读取stdin(刷题别再因为stdin踩坑了喂)&lt;/h2&gt;
&lt;p&gt;fmt包内 Scan系列 SScan系列 Fscan系列如下：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Golang schedule</title>
      <link>https://duck-dd.github.io/posts/go-schedule/</link>
      <pubDate>Sat, 21 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://duck-dd.github.io/posts/go-schedule/</guid>
      <description>&lt;h2 id=&#34;写在前面&#34;&gt;写在前面&lt;/h2&gt;
&lt;p&gt;runtime包实现了所有&lt;code&gt;goroutine scheduler&lt;/code&gt;、&lt;code&gt;memory allocator&lt;/code&gt;、&lt;code&gt;garbage collector&lt;/code&gt;细节，理论上可以从runtime包获取一切信息，没有直接怼源码，而是站在巨人的肩膀上（直接吃大佬们吃剩下的）。&lt;/p&gt;
&lt;p&gt;搜集到的材料，大家都是基于不同的go版本做的分析，而go版本迭代调度算法也在持续更新，所以整理的可能有些乱。但是可以保证的是，所有材料都是GM-&amp;gt;GMP演化后的材料。&lt;/p&gt;
&lt;h2 id=&#34;gm&#34;&gt;GM&lt;/h2&gt;
&lt;p&gt;go1.1版本以前，调度使用GM模型，如下图所示。简单的理解GM模型，就是有一个始终执行的调度函数schedule不停的执行调度计算，当某个M的G执行完成了，调度器就把这个M放回M队列，可绑定执行其他G（如果某个M+G发生了syscall，那么本来并发度是通过M数量控制的，此时并发度就降低了？）；如果G执行过程中创建新的G，会将新的G放入到G全局可执行队列中。G全局可执行队列的操作有一把全局锁，这导致了各个M对G全局队列的操作存在严重的竞争。&lt;/p&gt;
&lt;p&gt;下面这段完全是我的臆测，请别太相信：&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;简单概括呢，所以可以认为有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;G全局可执行队列(以下也可能简称G可执行队列)&lt;/li&gt;
&lt;li&gt;M可用队列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;调度器要做的事就是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从G的可执行队列取G并从M的可用队列取M，将二者绑定开始执行G&lt;/li&gt;
&lt;li&gt;对于已经执行完的G，销毁G并立即将M释放回M可用队列供后续使用&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img alt=&#34;GM-model&#34; loading=&#34;lazy&#34; src=&#34;https://duck-dd.github.io/images/go-schedule/gm.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;那么GM模型有哪些问题呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(&lt;strong&gt;重点问题&lt;/strong&gt;)单一的全局mutex(sched.lock)和集中状态管理
&lt;ul&gt;
&lt;li&gt;mutex需要保护所有与全局goroutine队列相关操作(创建、完成、重排等等)，竞争严重&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(&lt;strong&gt;重点问题&lt;/strong&gt;)per-M内存(M.mcache)问题
&lt;ul&gt;
&lt;li&gt;每个M都需要一个mcache，会导致资源消耗过大(每个mcache可以吸纳到2MB的内存缓存和其他缓存)
&lt;ul&gt;
&lt;li&gt;举个栗子，一个陷入syscall的M并不需要使用cache，但是在全部的M中，陷入系统调用的M与执行goroutine的M的比例可能是&lt;code&gt;N:1(N&amp;gt;&amp;gt;1)&lt;/code&gt;，这就导致了&lt;code&gt;N/(N+1)&lt;/code&gt;比例的mcache在闲置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据局部性差&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;举个栗子，M1执行G1，此时创建了G2，G2通常是立刻进入了G全局可执行队列，而此时M1还在执行G1，所以G2通常被其他M执行，但是G1和G2通常强相关，所以G2最好也在M1上执行，因为G2对M1的缓存命中率更高&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;goroutine传递问题
&lt;ul&gt;
&lt;li&gt;goroutine(G)交接(G.nextg)，M之间会经常交接可运行的G&lt;/li&gt;
&lt;li&gt;再通俗点说，就是G空转，本来能够好好在一个M上执行完，但是由于全局队列的存在，G一旦回全局队列了，下次就不知道被哪个M取走了，所以叫“空转”；M加载G的上下文是有开销的，所以空转会导致性能下降&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;频繁的线程阻塞/解阻塞
&lt;ul&gt;
&lt;li&gt;syscalls情况下，线程经常被阻塞和解阻塞，增加了很多额外开销&lt;/li&gt;
&lt;li&gt;通俗点说，M+G syscall，M阻塞，syscall完成后，M解阻塞继续执行G（如果是通过M数量控制并发度，这是不是就导致了并发度降低？）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gmp&#34;&gt;GMP&lt;/h2&gt;
&lt;p&gt;基于以上说的GM的问题，go1.1以后开始使用GMP调度模型。
G、M、P的定义如下(***/src/runtime/runtime2.go)。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;type g struct {
	// Stack parameters.
	// stack describes the actual stack memory: [stack.lo, stack.hi).
	// stackguard0 is the stack pointer compared in the Go stack growth prologue.
	// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.
	// stackguard1 is the stack pointer compared in the C stack growth prologue.
	// It is stack.lo+StackGuard on g0 and gsignal stacks.
	// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).
	stack       stack   // offset known to runtime/cgo
	stackguard0 uintptr // offset known to liblink
	stackguard1 uintptr // offset known to liblink

	_panic       *_panic // innermost panic - offset known to liblink
	_defer       *_defer // innermost defer
	m            *m      // current m; offset known to arm liblink
	sched        gobuf
	syscallsp    uintptr        // if status==Gsyscall, syscallsp = sched.sp to use during gc
	syscallpc    uintptr        // if status==Gsyscall, syscallpc = sched.pc to use during gc
	stktopsp     uintptr        // expected sp at top of stack, to check in traceback
	param        unsafe.Pointer // passed parameter on wakeup
	atomicstatus uint32
	stackLock    uint32 // sigprof/scang lock; TODO: fold in to atomicstatus
	goid         int64
	schedlink    guintptr
	waitsince    int64      // approx time when the g become blocked
	waitreason   waitReason // if status==Gwaiting

	preempt       bool // preemption signal, duplicates stackguard0 = stackpreempt
	preemptStop   bool // transition to _Gpreempted on preemption; otherwise, just deschedule
	preemptShrink bool // shrink stack at synchronous safe point

	// asyncSafePoint is set if g is stopped at an asynchronous
	// safe point. This means there are frames on the stack
	// without precise pointer information.
	asyncSafePoint bool

	paniconfault bool // panic (instead of crash) on unexpected fault address
	gcscandone   bool // g has scanned stack; protected by _Gscan bit in status
	throwsplit   bool // must not split stack
	// activeStackChans indicates that there are unlocked channels
	// pointing into this goroutine&amp;#39;s stack. If true, stack
	// copying needs to acquire channel locks to protect these
	// areas of the stack.
	activeStackChans bool
	// parkingOnChan indicates that the goroutine is about to
	// park on a chansend or chanrecv. Used to signal an unsafe point
	// for stack shrinking. It&amp;#39;s a boolean value, but is updated atomically.
	parkingOnChan uint8

	raceignore     int8     // ignore race detection events
	sysblocktraced bool     // StartTrace has emitted EvGoInSyscall about this goroutine
	sysexitticks   int64    // cputicks when syscall has returned (for tracing)
	traceseq       uint64   // trace event sequencer
	tracelastp     puintptr // last P emitted an event for this goroutine
	lockedm        muintptr
	sig            uint32
	writebuf       []byte
	sigcode0       uintptr
	sigcode1       uintptr
	sigpc          uintptr
	gopc           uintptr         // pc of go statement that created this goroutine
	ancestors      *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors)
	startpc        uintptr         // pc of goroutine function
	racectx        uintptr
	waiting        *sudog         // sudog structures this g is waiting on (that have a valid elem ptr); in lock order
	cgoCtxt        []uintptr      // cgo traceback context
	labels         unsafe.Pointer // profiler labels
	timer          *timer         // cached timer for time.Sleep
	selectDone     uint32         // are we participating in a select and did someone win the race?

	// Per-G GC state

	// gcAssistBytes is this G&amp;#39;s GC assist credit in terms of
	// bytes allocated. If this is positive, then the G has credit
	// to allocate gcAssistBytes bytes without assisting. If this
	// is negative, then the G must correct this by performing
	// scan work. We track this in bytes to make it fast to update
	// and check for debt in the malloc hot path. The assist ratio
	// determines how this corresponds to scan work debt.
	gcAssistBytes int64
}

type m struct {
	g0      *g     // goroutine with scheduling stack
	morebuf gobuf  // gobuf arg to morestack
	divmod  uint32 // div/mod denominator for arm - known to liblink

	// Fields not known to debuggers.
	procid        uint64       // for debuggers, but offset not hard-coded
	gsignal       *g           // signal-handling g
	goSigStack    gsignalStack // Go-allocated signal handling stack
	sigmask       sigset       // storage for saved signal mask
	tls           [6]uintptr   // thread-local storage (for x86 extern register)
	mstartfn      func()
	curg          *g       // current running goroutine
	caughtsig     guintptr // goroutine running during fatal signal
	p             puintptr // attached p for executing go code (nil if not executing go code)
	nextp         puintptr
	oldp          puintptr // the p that was attached before executing a syscall
	id            int64
	mallocing     int32
	throwing      int32
	preemptoff    string // if != &amp;#34;&amp;#34;, keep curg running on this m
	locks         int32
	dying         int32
	profilehz     int32
	spinning      bool // m is out of work and is actively looking for work
	blocked       bool // m is blocked on a note
	newSigstack   bool // minit on C thread called sigaltstack
	printlock     int8
	incgo         bool   // m is executing a cgo call
	freeWait      uint32 // if == 0, safe to free g0 and delete m (atomic)
	fastrand      [2]uint32
	needextram    bool
	traceback     uint8
	ncgocall      uint64      // number of cgo calls in total
	ncgo          int32       // number of cgo calls currently in progress
	cgoCallersUse uint32      // if non-zero, cgoCallers in use temporarily
	cgoCallers    *cgoCallers // cgo traceback if crashing in cgo call
	park          note
	alllink       *m // on allm
	schedlink     muintptr
	lockedg       guintptr
	createstack   [32]uintptr // stack that created this thread.
	lockedExt     uint32      // tracking for external LockOSThread
	lockedInt     uint32      // tracking for internal lockOSThread
	nextwaitm     muintptr    // next m waiting for lock
	waitunlockf   func(*g, unsafe.Pointer) bool
	waitlock      unsafe.Pointer
	waittraceev   byte
	waittraceskip int
	startingtrace bool
	syscalltick   uint32
	freelink      *m // on sched.freem

	// these are here because they are too large to be on the stack
	// of low-level NOSPLIT functions.
	libcall   libcall
	libcallpc uintptr // for cpu profiler
	libcallsp uintptr
	libcallg  guintptr
	syscall   libcall // stores syscall parameters on windows

	vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call)
	vdsoPC uintptr // PC for traceback while in VDSO call

	// preemptGen counts the number of completed preemption
	// signals. This is used to detect when a preemption is
	// requested, but fails. Accessed atomically.
	preemptGen uint32

	// Whether this is a pending preemption signal on this M.
	// Accessed atomically.
	signalPending uint32

	dlogPerM

	mOS

	// Up to 10 locks held by this m, maintained by the lock ranking code.
	locksHeldLen int
	locksHeld    [10]heldLockInfo
}

type p struct {
	id          int32
	status      uint32 // one of pidle/prunning/...
	link        puintptr
	schedtick   uint32     // incremented on every scheduler call
	syscalltick uint32     // incremented on every system call
	sysmontick  sysmontick // last tick observed by sysmon
	m           muintptr   // back-link to associated m (nil if idle)
	mcache      *mcache
	pcache      pageCache
	raceprocctx uintptr

	deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)
	deferpoolbuf [5][32]*_defer

	// Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen.
	goidcache    uint64
	goidcacheend uint64

	// Queue of runnable goroutines. Accessed without lock.
	runqhead uint32
	runqtail uint32
	runq     [256]guintptr
	// runnext, if non-nil, is a runnable G that was ready&amp;#39;d by
	// the current G and should be run next instead of what&amp;#39;s in
	// runq if there&amp;#39;s time remaining in the running G&amp;#39;s time
	// slice. It will inherit the time left in the current time
	// slice. If a set of goroutines is locked in a
	// communicate-and-wait pattern, this schedules that set as a
	// unit and eliminates the (potentially large) scheduling
	// latency that otherwise arises from adding the ready&amp;#39;d
	// goroutines to the end of the run queue.
	runnext guintptr

	// Available G&amp;#39;s (status == Gdead)
	gFree struct {
		gList
		n int32
	}

	sudogcache []*sudog
	sudogbuf   [128]*sudog

	// Cache of mspan objects from the heap.
	mspancache struct {
		// We need an explicit length here because this field is used
		// in allocation codepaths where write barriers are not allowed,
		// and eliminating the write barrier/keeping it eliminated from
		// slice updates is tricky, moreso than just managing the length
		// ourselves.
		len int
		buf [128]*mspan
	}

	tracebuf traceBufPtr

	// traceSweep indicates the sweep events should be traced.
	// This is used to defer the sweep start event until a span
	// has actually been swept.
	traceSweep bool
	// traceSwept and traceReclaimed track the number of bytes
	// swept and reclaimed by sweeping in the current sweep loop.
	traceSwept, traceReclaimed uintptr

	palloc persistentAlloc // per-P to avoid mutex

	_ uint32 // Alignment for atomic fields below

	// The when field of the first entry on the timer heap.
	// This is updated using atomic functions.
	// This is 0 if the timer heap is empty.
	timer0When uint64

	// Per-P GC state
	gcAssistTime         int64    // Nanoseconds in assistAlloc
	gcFractionalMarkTime int64    // Nanoseconds in fractional mark worker (atomic)
	gcBgMarkWorker       guintptr // (atomic)
	gcMarkWorkerMode     gcMarkWorkerMode

	// gcMarkWorkerStartTime is the nanotime() at which this mark
	// worker started.
	gcMarkWorkerStartTime int64

	// gcw is this P&amp;#39;s GC work buffer cache. The work buffer is
	// filled by write barriers, drained by mutator assists, and
	// disposed on certain GC state transitions.
	gcw gcWork

	// wbBuf is this P&amp;#39;s GC write barrier buffer.
	//
	// TODO: Consider caching this in the running G.
	wbBuf wbBuf

	runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point

	// Lock for timers. We normally access the timers while running
	// on this P, but the scheduler can also do it from a different P.
	timersLock mutex

	// Actions to take at some time. This is used to implement the
	// standard library&amp;#39;s time package.
	// Must hold timersLock to access.
	timers []*timer

	// Number of timers in P&amp;#39;s heap.
	// Modified using atomic instructions.
	numTimers uint32

	// Number of timerModifiedEarlier timers on P&amp;#39;s heap.
	// This should only be modified while holding timersLock,
	// or while the timer status is in a transient state
	// such as timerModifying.
	adjustTimers uint32

	// Number of timerDeleted timers in P&amp;#39;s heap.
	// Modified using atomic instructions.
	deletedTimers uint32

	// Race context used while executing timer functions.
	timerRaceCtx uintptr

	// preempt is set to indicate that this P should be enter the
	// scheduler ASAP (regardless of what G is running on it).
	preempt bool

	pad cpu.CacheLinePad
}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;gmp模型的一些概念&#34;&gt;GMP模型的一些概念&lt;/h3&gt;
&lt;p&gt;上面M中有两个g需要关注下，curg和g0。
curg就是M当前绑定的G。
g0是带有调度栈的goroutine，普通的G的栈是分配在堆上的可增长的栈，而g0的栈是M对应的线程的栈。所有调度相关的代码，会先切换到该goroutine的栈中执行。即，线程的栈也是用的g实现，而不是使用的OS。&lt;/p&gt;</description>
    </item>
    <item>
      <title>DNS</title>
      <link>https://duck-dd.github.io/posts/dns/</link>
      <pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate>
      <guid>https://duck-dd.github.io/posts/dns/</guid>
      <description>&lt;h1 id=&#34;dns挺好啥是dns&#34;&gt;DNS，挺好，啥是DNS？&lt;/h1&gt;
&lt;h2 id=&#34;一些概念&#34;&gt;一些概念&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;FQDN: Fully Qualified Domain Name，全限定域名，同时带有主机名和域名的名称（通过符号“.”），例如一个FQDN是www.baidu.com，www是主机名，baidu.com是域名。再举例，我是海淀吴彦祖，你是**吴彦祖，但是我们还知道，有个人就叫吴彦祖，这么多吴彦祖我们都没有混淆，因为名字前面加上了地域，也就是域名。从逻辑上看FQDN，就是主机名的完整表达，类似绝对路径，通过一个FQDN我们可以在全网内锁定主机位置。&lt;/li&gt;
&lt;li&gt;cache only DNS server : 有.的zone file的DNS服务器，本身没有任何解析数据，完全靠查询来获取数据源&lt;/li&gt;
&lt;li&gt;forwarding DNS server : 连.的zone file都没有，完全靠向上层查询获取数据；当使用forwarding功能时，即使本身有.的zone file，也不会向.查询，该DNS server还是会将查询完全委托给上层。&lt;/li&gt;
&lt;li&gt;CIDR:Classless Inter-Domain Routing, 无类域间路由，不按固定的字节来划分网络编号，可以使用IP地址中任何相邻位的数字作为网络编号，例如某机构需要2个B类网络大小的空间，那么可以使用前15位作为网络编号，例如127.127.0.0/15
&lt;ul&gt;
&lt;li&gt;A类,B类,C类网络：A类网络以IP地址的第一个字节(前8位)作为网络编号,剩下的24位为主机;B类网络前两个字节为网络编号;C类网络前三个字节为网络编号&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dns做什么&#34;&gt;DNS做什么&lt;/h2&gt;
&lt;p&gt;ipv4 32bit，ipv6 128bit，即使转成10/16进制也没人记得住，但是人类的头脑善于记录名字，所以可以搞一个名字跟IP对应，名字跟IP的对应关系解析，就是DNS提供的服务。&lt;/p&gt;
&lt;p&gt;主机名的解析有一个发展的过程。&lt;/p&gt;
&lt;p&gt;最初没有DNS人们如何记住各个服务名字跟IP的对应关系呢？就是写在/etc/hosts文件里，自己写麻烦，那就统一写到中心，使用的时候从中心拉取。&lt;/p&gt;
&lt;p&gt;这个中心就是internic，主机名IP对应关系修改时，注册到internic中；用户准备上网之前先去internic把最新的文件拉下来，放在自己的/etc/hosts。&lt;/p&gt;
&lt;p&gt;这种方式问题很多：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例如internic拉取的文件会很大，每次打开电脑先拉个100G的文件然后再开始上网，就算你磁盘扛得住，你的网络不够好也很难受（因为你不能明确说明自己上网需要的主机名，只能拉全量，互联网业务增长该文件会越来越大）&lt;/li&gt;
&lt;li&gt;例如这种方式是静态的，需要用户主动触发更新行为（你总不能让一个人开机默认就去下载100G的文件，他不可能给你授权的）；试想一下，一个网瘾少年下午6点睡眼惺忪的起床，打开电脑先从internic下载了半个小时文件，期间去洗漱吃了早饭，然后开开心心开始打游戏，突然，游戏掉线了，上贴吧一查，大家都说快去重新拉取internic的hosts文件呀，游戏域名被友商攻击换域名了，这还好，少年骂骂咧咧下载个文件就完事了，要是贴吧都上不去就更让人懵逼了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;时代的进步总是聪明人推动的。Berkeley一个同学就觉得这种方式不太行，于是他就搞出一套BIND系统提供DNS服务。&lt;/p&gt;
&lt;h2 id=&#34;bindberkeley-internet-name-domain&#34;&gt;BIND，Berkeley Internet Name Domain&lt;/h2&gt;
&lt;h3 id=&#34;bind管理方式&#34;&gt;BIND管理方式&lt;/h3&gt;
&lt;p&gt;BIND是一套阶层式的管理主机名与IP对应关系的系统。&lt;/p&gt;
&lt;p&gt;阶层式？可以简单理解为树状结构的不同层级，下面来简单分析下阶层式。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://duck-dd.github.io/images/dns/1.jpg&#34;&gt;&lt;/p&gt;
&lt;p&gt;以www.baidu.com为例，最上层根服务器，domain name是&amp;quot;.&amp;quot;，然后有三个hostname &amp;ldquo;net com cn&amp;rdquo;，再到第三层，hostname分别是pdd baidu tencent，此时domain name为.com. ，以此类推。。。需要注意的是，不是每一个&amp;quot;.&amp;ldquo;都拆分domain name&amp;amp;hostname，例如上图  video.www.baidu.com ，其中domain name为baidu.com.，hostname为video.www。按照上述方式分层，每一个服务节点（权威）只负责自己的一小撮域名，这就避免了大量数据集中的问题。&lt;/p&gt;
&lt;p&gt;DNS阶层系统的最上方是一个&amp;rdquo;.&amp;quot;，root，是根服务器，本质上讲，这里的&amp;quot;.&amp;ldquo;后面其实是空标签，这是为root保留的；根服务器下一层管理的是Top Level Domains(TLD)，例如com. net. org.等等。&lt;/p&gt;
&lt;p&gt;每个上一层的DNS服务器，所记录的信息，只有下一层的主机名；再下一层，授权给再再下层某个主机管理，这就是分层管理；DNS分层最多到127层(实际上不会用到这么多)，每一层最多63个字符(不包括&amp;rdquo;.&amp;quot;)；同一层内不允许同名，确保唯一性。&lt;/p&gt;
&lt;h3 id=&#34;bind查询流程&#34;&gt;BIND查询流程&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;当浏览器输入 &lt;a href=&#34;https://www.baidu.com&#34;&gt;https://www.baidu.com&lt;/a&gt; ，先查浏览器缓存，再查/etc/hosts文件，都找不到www.baidu.com的解析时，会根据/etc/resolv.conf文件内配置的DNS服务器地址，去进行DNS解析，询问www.baidu.com的A记录&lt;/li&gt;
&lt;li&gt;client第一步找到的DNS服务器通常为运营商提供的local DNS服务器，local DNS作为名称服务器，接收client端的递归查询请求，若local DNS服务器自身没有www.baidu.com的解析结果，则向.DNS服务器发起解析请求，询问www.baidu.com是啥A记录呢？
&lt;ul&gt;
&lt;li&gt;其实递归查询实际过程中，local DNS若未命中缓存，并不是直接查询根服务器，他会寻找已知最近的名称服务器(待实验确认)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;.并不知道www.baidu.com的IP，它会告诉你我只知道.com，IP给你，你去问它吧&lt;/li&gt;
&lt;li&gt;然后local DNS获取到了.com的信息后，开始向.com询问www.baidu.com的解析结果&lt;/li&gt;
&lt;li&gt;.com也不知道www.baidu.com的IP地址，它会说，我只认识baidu.com，你去问它吧
&lt;ul&gt;
&lt;li&gt;.com返回的一般是baidu.com的多个NS域名(及其IP,胶水记录)，如下例图，那么如何选择权威呢？BIND名称服务器使用RTT(roundtrip time)的度量方式来选择对同一区域中的名称服务器进行选择，即选择RTT最小的那个名称服务器(dig +trace抓包并没看到对RTT的探测，现象上看是从ns*.baidu.com里面随机选择的？)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://duck-dd.github.io/images/dns/2.jpg&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
