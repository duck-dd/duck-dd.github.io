<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Language on Home Page</title>
    <link>https://duck-dd.github.io/tags/language/</link>
    <description>Recent content in Language on Home Page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ztq.</copyright>
    <lastBuildDate>Sat, 21 Aug 2021 22:27:31 +0800</lastBuildDate><atom:link href="https://duck-dd.github.io/tags/language/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Golang schedule</title>
      <link>https://duck-dd.github.io/posts/golang-schedule/</link>
      <pubDate>Sat, 21 Aug 2021 22:27:31 +0800</pubDate>
      
      <guid>https://duck-dd.github.io/posts/golang-schedule/</guid>
      <description>写在前面 如果哪里写错辣了您的眼睛，请务必评论区批评指正，感激不尽。
留了一些TODO。
runtime包实现了所有goroutine scheduler、memory allocator、garbage collector细节，理论上可以从runtime包获取一切信息，没有直接怼源码，而是站在巨人的肩膀上（直接吃大佬们吃剩下的），看大家的各种说法，整理一下。
搜集到的材料，大家都是基于不同的go版本做的分析，而go版本迭代调度算法也在持续更新，所以整理的可能有些乱。但是可以保证的是，所有材料都是GM-&amp;gt;GMP演化后的材料。
GM go1.1版本以前，调度使用GM模型，如下图所示。简单的理解GM模型，就是有一个始终执行的调度函数schedule不停的执行调度计算，当某个M的G执行完成了，调度器就把这个M放回M队列，可绑定执行其他G（如果某个M+G发生了syscall，那么本来并发度是通过M数量控制的，此时并发度就降低了？）；如果G执行过程中创建新的G，会将新的G放入到G全局可执行队列中。G全局可执行队列的操作有一把全局锁，这导致了各个M对G全局队列的操作存在严重的竞争。
下面这段完全是我的臆测，请别太相信：
 简单概括呢，所以可以认为有：
 G全局可执行队列(以下也可能简称G可执行队列) M可用队列  调度器要做的事就是：
 从G的可执行队列取G并从M的可用队列取M，将二者绑定开始执行G 对于已经执行完的G，销毁G并立即将M释放回M可用队列供后续使用   那么GM模型有哪些问题呢？
 (重点问题)单一的全局mutex(sched.lock)和集中状态管理  mutex需要保护所有与全局goroutine队列相关操作(创建、完成、重排等等)，竞争严重   (重点问题)per-M内存(M.mcache)问题  每个M都需要一个mcache，会导致资源消耗过大(每个mcache可以吸纳到2MB的内存缓存和其他缓存)  举个栗子，一个陷入syscall的M并不需要使用cache，但是在全部的M中，陷入系统调用的M与执行goroutine的M的比例可能是N:1(N&amp;gt;&amp;gt;1)，这就导致了N/(N+1)比例的mcache在闲置   数据局部性差:  举个栗子，M1执行G1，此时创建了G2，G2通常是立刻进入了G全局可执行队列，而此时M1还在执行G1，所以G2通常被其他M执行，但是G1和G2通常强相关，所以G2最好也在M1上执行，因为G2对M1的缓存命中率更高     goroutine传递问题  goroutine(G)交接(G.nextg)，M之间会经常交接可运行的G 再通俗点说，就是G空转，本来能够好好在一个M上执行完，但是由于全局队列的存在，G一旦回全局队列了，下次就不知道被哪个M取走了，所以叫“空转”；M加载G的上下文是有开销的，所以空转会导致性能下降   频繁的线程阻塞/解阻塞  syscalls情况下，线程经常被阻塞和解阻塞，增加了很多额外开销 通俗点说，M+G syscall，M阻塞，syscall完成后，M解阻塞继续执行G（如果是通过M数量控制并发度，这是不是就导致了并发度降低？）    GMP 基于以上说的GM的问题，go1.1以后开始使用GMP调度模型。 G、M、P的定义如下(***/src/runtime/runtime2.go)。
type g struct { // Stack parameters. // stack describes the actual stack memory: [stack.</description>
    </item>
    
    <item>
      <title>Golang Tips For Me (continuously updated)</title>
      <link>https://duck-dd.github.io/posts/go-tips/</link>
      <pubDate>Fri, 02 Jul 2021 23:17:21 +0800</pubDate>
      
      <guid>https://duck-dd.github.io/posts/go-tips/</guid>
      <description>说明：
持续更新(标题含TODO关键字的小节，都会以topic开始，后续会持续完善topic)。记录内容是一些对我而言：
 小的骚操作(可能也包含其背后的实现原理) 容易理解偏差的知识点 冷门的知识点   Golang代码执行顺序 没有并发，一个顺序逻辑，CPU真正执行指令不一定与编码顺序完全一致，Go的编译器会做优化，前提是会解决依赖逻辑，看起来是“顺序执行”这一假设。
了解更多可查看Golang内存模型规范。
 internal包 internal包，只能被和internal目录有同一个父目录的包所导入。 例如，net/http/internal/chunked内部包只能被net/http/httputil或net/http包导入，但是不能被net/url包导入。不过net/url包却可以导入net/http/httputil包。
 变量交换 i, j = j, i // 交换 i 和 j 的值  for循环有其他识别break的关键字 for循环内有其他识别break的关键字时，其他关键字内的break会被其识别而不会跳出for，以下用select举例，switch同理。
for { select { case &amp;lt;-sigChan: // exit for break default: // do something } } 以上break并不能退出for循环，可以使用标签或goto解决：
// 1 标签 FOR: for { select { case &amp;lt;-sigChan: // exit for break FOR default: // do something } } ---------------------------- // 2 goto for { select { case &amp;lt;-sigChan: // exit for goto ENDFOR default: // do something } } ENDFOR:  读取stdin(刷题别再因为stdin踩坑了喂) fmt包内 Scan系列 SScan系列 Fscan系列如下：</description>
    </item>
    
  </channel>
</rss>
